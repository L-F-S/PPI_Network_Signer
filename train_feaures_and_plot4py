# -*- coding: utf-8 -*-
"""
Created on Mon May 10 15:32:32 2021

@author: los4
3.5 removed p_complex validations
"""

import os
import pandas  as pd
import numpy  as np
from datetime import date
from statsmodels.stats.proportion import proportions_ztest
import matplotlib.pyplot as plt
from preproc_utils import load_training_data
from train_and_vis4 import k_fold, read_features, log, load_features, AUCs
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import pickle

##############################################################################
#  INPUTS
##############################################################################
# SPECIES = "H_sapiens"#
SPECIES =  "S_cerevisiae" #

DRIVE_PATH  =  "G:" +os.sep+"My Drive"+ os.sep +"SECRET-ITN" + os.sep +"Projects" + os.sep 
# DRIVE_PATH  =  "G:" +os.sep+"Il mio Drive"+ os.sep +"SECRET-ITN" + os.sep +"Projects" + os.sep 

HOME_DIR=DRIVE_PATH+'network_signing'+os.sep
MAIN_DATA_DIR = DRIVE_PATH+'Data'+os.sep+SPECIES+os.sep
# Inputs directories
EDGES_DIR = HOME_DIR+'input'+os.sep+SPECIES+os.sep +'edges'+os.sep
NET_DIR = HOME_DIR+'input'+os.sep+SPECIES+os.sep +'network'+os.sep
KO_DIR = HOME_DIR+'input'+os.sep+SPECIES+os.sep +'signatures'+os.sep
LBL_DIR = HOME_DIR+'input'+os.sep+SPECIES+os.sep +'labels'+os.sep

#SIGNAL output directories
FT_DIR=HOME_DIR+'output'+os.sep+SPECIES+os.sep+os.sep+'features'+os.sep
SIGNAL_DIR=HOME_DIR+'output'+os.sep+SPECIES+os.sep+os.sep+'predictions'+os.sep
MOD_DIR=HOME_DIR+'output'+os.sep+SPECIES+os.sep+os.sep+'models'+os.sep
IMG_DIR=HOME_DIR+os.sep+'imgs'+os.sep+'v4'+os.sep+SPECIES+os.sep

os.chdir(HOME_DIR)

logfile=HOME_DIR+os.sep+'validation_out'+os.sep+'crossvalidationslog.txt'
DATE=str(date.today())

mean_AUROC_of = {}
mean_AUPR_of = {}
std_AUROC_of = {}
std_AUPR_of = {}
if SPECIES == 'S_cerevisiae':
    datasets = ['patkar_kegg','kpi','ubiq']#, 
    pert_map = 'Holstege'#'reimand'
else:
    datasets = ['ubinet2','PSP','depod'] #'kegg'
    pert_map =  'CMGE' #'ADPBH_1000'  ##   human: 'K562gwps''ADPBH_1 'ADPBH_1000'
dataset_name = '_'.join(datasets)
sampling = ''#'_DS'
finaldataname=dataset_name+'_'+pert_map+sampling
comments=sampling
#%%  merge selected datasets and perform crossvalidation
signed_datasets_labels, _ = load_training_data(LBL_DIR, datasets, SPECIES)
# signed_datasets_labels['kegg'] = signed_datasets_labels['kegg'].iloc[1:]#temp todo: only for kegg CMGE che x qlc motivo la features table manca el primo coos
signed_datasets_features = load_features(FT_DIR, [i+'_'+pert_map for i in datasets], SPECIES) #attenzione, aggiungi _pert_map se non e' hosltege


training_labels=pd.concat(list(signed_datasets_labels.values())) # works because python dictionaries are ordered in python3
features_table= pd.concat(list(signed_datasets_features.values()))

#%% #%%  12 01 2023 features knockout validation
with open(KO_DIR+'plus_targets'+'_'+pert_map+'.pkl', 'rb') as f:
    plus_targets_of_deletion=pickle.load( f)
with open(KO_DIR+'minus_targets'+'_'+pert_map+'.pkl','rb') as f:
    minus_targets_of_deletion=pickle.load(f)
#%%
# if plus_targets_of_deletion.keys() == minus_targets_of_deletion.keys(): #doublecheck
#     val_ko=int(len(plus_targets_of_deletion)*0.2)
#     features=len(features_table.columns)
#     plus_feat_cols=features_table.columns[0:int(features/2)] 
#     minus_feat_cols=features_table.columns[int(features/2):]
    
#     test_plus_features = features_table[features_table.columns[:int(features/2)-val_ko]]
#     validation_plus_features = features_table[features_table.columns[int(features/2)-val_ko:int(features/2)]]
    
#     test_minus_features = features_table[features_table.columns[int(features/2):features-val_ko]]
#     validation_minus_features = features_table[features_table.columns[int(features)-val_ko:]]
# features_table=pd.concat([test_plus_features,test_minus_features],axis=1)
#%% human: 66 features >=1000 targets + t>1
# asd=signed_datasets_features = load_features(INPUTDIR, [i+'_ADPBH_1' for i in datasets], SPECIES)
# features_table1=pd.concat(list(asd.values()))
# with open(INPUTDIR+'plus_targets'+'_ADPBH_1.pkl', 'rb') as f:
#     plus_1=pickle.load( f)
# with open(INPUTDIR+'plus_targets'+'_ADPBH_1000.pkl', 'rb') as f:
#     plus_1000=pickle.load( f)
# with open(INPUTDIR+'minus_targets'+'_ADPBH_1.pkl','rb') as f:
#     minus_1=pickle.load(f)
# with open(INPUTDIR+'minus_targets'+'_'+pert_map+'.pkl','rb') as f:
#     minus_1000=pickle.load(f)
# import collections
# common_keys = collections.defaultdict(list)
# for i,k in enumerate(plus_1.keys()):
#     if k in plus_1000.keys():
#         common_keys[k].append([i])
# for j, k in enumerate(plus_1000.keys()):
#     if k in common_keys.keys():
#         common_keys[k].append(j)
# print(len(common_keys.keys())) #66
#%%
#11091: [[387], 388]
# same_cols=[]
# for c in features_table.columns:
#     for c1 in features_table1.columns:
#         if features_table[c][features_table[c]==features_table1[c1]].sum() > 0:
#             print(c,c1)
#             same_cols.append((c,c1))
            

#i think they should be the same ma non lo sono
#%% merged datasets CV:
classifier=RandomForestClassifier()#LogisticRegression(max_iter=1000)#
classifier, mean_fpr, mean_tpr, tprs_lower, tprs_upper,\
    mean_AUROC_of[finaldataname], std_AUROC_of[finaldataname] =k_fold(classifier,5,\
                            features_table,training_labels, finaldataname, \
                                IMG_DIR, metric="ROC", downsampling=sampling,\
                                    plot=True) 

classifier, mean_fpr, mean_tpr, tprs_lower, tprs_upper,\
    mean_AUPR_of[finaldataname], std_AUPR_of[finaldataname]=k_fold(classifier,5,\
                            features_table,training_labels, finaldataname,\
                                IMG_DIR, metric="PrecRec", downsampling=sampling,
                                plot=True)
mean_AUPR_of[finaldataname]=0
std_AUPR_of[finaldataname]=0
print("ROC AUC:", finaldataname, np.round(mean_AUROC_of[finaldataname],2))
print("PrecRec AUC:", finaldataname, np.round(mean_AUPR_of[finaldataname],2))
#%%
log(dataset_name,np.round(mean_AUROC_of[finaldataname],2),\
    np.round(mean_AUPR_of[finaldataname],2),logfile, DATE, SPECIES,\
            pert_map=pert_map, comments=comments)
with open(MOD_DIR+finaldataname+'.rf', 'wb') as f:
    pickle.dump(classifier, f)
#%% dataset_wise CV:
### 
for dataset_name, training_labels in signed_datasets_labels.items():
    print(dataset_name)
    finaldataname=dataset_name+'_'+pert_map+sampling
    current_features_table = signed_datasets_features[dataset_name+"_"+pert_map]
    classifier=RandomForestClassifier()#LogisticRegression(max_iter=1000)#
    classifier, mean_fpr, mean_tpr, tprs_lower, tprs_upper,\
        mean_AUROC_of[finaldataname], std_AUROC_of[finaldataname] =k_fold(classifier,5,\
                                current_features_table,training_labels, finaldataname, \
                                    IMG_DIR, metric="ROC", downsampling=sampling,\
                                        plot=False) 
    
    # classifier, mean_fpr, mean_tpr, tprs_lower, tprs_upper,\
    #     mean_AUPR_of[finaldataname], std_AUPR_of[finaldataname]=k_fold(classifier,5,\
    #                             current_features_table,training_labels, finaldataname,\
    #                                 IMG_DIR, metric="PrecRec", downsampling=sampling,
    #                                 plot=True)
    mean_AUPR_of[finaldataname]=0
    std_AUPR_of[finaldataname]=0
    print("ROC AUC:", finaldataname, np.round(mean_AUROC_of[finaldataname],2))
    print("PrecRec AUC:", finaldataname, np.round(mean_AUPR_of[finaldataname],2))
    
    log(dataset_name,np.round(mean_AUROC_of[finaldataname],2),\
        np.round(mean_AUPR_of[finaldataname],2),logfile, DATE, SPECIES,\
            pert_map=pert_map, comments=comments)
    
    with open(MOD_DIR+finaldataname+'.rf', 'wb') as f:
        pickle.dump(classifier, f)
#%% 
with open(MOD_DIR+finaldataname+'.rf','rb') as f:
    classifier=pickle.load(f)
